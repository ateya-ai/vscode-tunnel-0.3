{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8919ae0-b14d-4ae2-9f5f-9637b37b0e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Develop Databricks Projects using VS Code\n",
    "## Disclaimer\n",
    "\n",
    " * The product may change or may never be released;\n",
    " * While we will not charge separately for this product right now, we may charge for it in the future. You will still incur charges for DBUs.\n",
    " * There's no formal support or SLAs for the preview - so please reach out to your account or other contact with any questions or feedback; and\n",
    " * We may terminate the preview or your access at any time;\n",
    " Place this notebook into a workspace folder and execute it in order to develop on the folder using a VSCode remote tunnel.\n",
    "\n",
    "## Requirements:\n",
    " * UC\n",
    " * DBR/MLR >= 14.x\n",
    " * Single user cluster\n",
    "\n",
    "## How to use\n",
    " * Place this notebook into the workspace folder you want to develop using VS Code\n",
    " * Configure\n",
    "   * Run the cell called \"Widgets\"\n",
    "   * Configure the IDE using the widgets at the top of the notebook\n",
    " * Click \"Run all\" each time the cluster is restarted.\n",
    " * Follow instructions in the output of the last cell to open the IDE\n",
    "   * Open project folder in the IDE\n",
    "   * Select python environment by executing the `Python: Select Interpreter` command. Select the item labeled `python.defaultInterpreterPath`.\n",
    "   * When using notebooks click `select kernel`, select `Python environments...`, and then select the highlighted environment\n",
    " * Click \"Interrupt\" to close the tunnel once you are done developing\n",
    "\n",
    "### Features\n",
    " * Authentication with `Github` and `Microsoft Entra ID` (can be configured in a widget)\n",
    "   * Microsoft authentication currently requires VS Code Insiders channel\n",
    " * Full debugger support for Python files\n",
    " * Jupyter notebooks\n",
    " * Debugger works in notebooks\n",
    " * Access to all cluster libraries\n",
    " * Full Python LSP including code completion for `spark`\n",
    " * UC support\n",
    " * Auto-termination after 10m (can be configured using a widget)\n",
    " * Toggle between `stable` and `insider` VS Code channels\n",
    " * Customized Jupyter kernel to support\n",
    "   * `spark` and `dbutils` globals\n",
    "   * `%sql`\n",
    "   * Better table output rendering\n",
    "\n",
    "### Limitations\n",
    " * Requires UC and DRB >= 14.x (because of SparkConnect) \n",
    " * Only SparkConnect and DBUtils from SparkConnect\n",
    " * No dbutils widgets\n",
    " * Notebooks written in VSCode don't show up in Databricks (needs DBR >= 17.2)\n",
    " * Notebooks written in Databricks can't be opened in VSCode (needs DBR >= 16)\n",
    " * No git support in VS Code. Code needs to be committed from Databricks (WIP)\n",
    " * VS Code\n",
    "   * Extensions defined in `.vscode/extensions.json` will be installed automatically. Manually installed extensions must be added to `.vscode/extensions.json` or they will be lost on cluster termination\n",
    "   * Python virtual environment needs to be selected manually using `Python: Select Interpreter` command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0a30b0-20c3-4308-aaf8-9d9adefbe661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download and install\n",
    "\n",
    "First down load and install the VS Code tunnel CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfb5024-b1c6-4b0c-94f5-131a1da2894e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Widgets"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate dbutils widget dropdown\n",
    "dbutils.widgets.dropdown(\"Provider\", \"microsoft\", [\"github\", \"microsoft\"])\n",
    "dbutils.widgets.dropdown(\"Duration\", \"10m\", [\"10m\", \"30m\", \"1h\", \"4h\"])\n",
    "dbutils.widgets.dropdown(\"VS Code Channel\", \"stable\", [\"stable\", \"insider\"])\n",
    "dbutils.widgets.dropdown(\"Create example.ipynb\", \"Yes\", [\"Yes\", \"No\"])\n",
    "\n",
    "import os\n",
    "\n",
    "PROVIDER = dbutils.widgets.get(\"Provider\")\n",
    "DURATION = dbutils.widgets.get(\"Duration\")\n",
    "CHANNEL = dbutils.widgets.get(\"VS Code Channel\")\n",
    "CREATE_EXAMPLE_NOTEBOOK = dbutils.widgets.get(\"Create example.ipynb\") == \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc51a2f-54f9-4a2f-8099-c2e25b594bf8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download VS Code CLI"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "import io\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "code_path = expanduser(\"~/code\")\n",
    "\n",
    "def rm_file(path):\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "\n",
    "\n",
    "def download_code(channel):\n",
    "    rm_file(f\"{code_path}/code\")\n",
    "    rm_file(f\"{code_path}/code-insiders\")\n",
    "\n",
    "    download_url = (\n",
    "        f\"https://code.visualstudio.com/sha/download?build={channel}&os=cli-alpine-x64\"\n",
    "    )\n",
    "    response = requests.get(download_url)\n",
    "\n",
    "    response.raise_for_status()  # Check that the request was successful\n",
    "\n",
    "    # Create a file-like object from the response content\n",
    "    file_like_object = io.BytesIO(response.content)\n",
    "\n",
    "    # Open the tar file\n",
    "    with tarfile.open(fileobj=file_like_object, mode=\"r:gz\") as tar:\n",
    "        tar.extractall(code_path)\n",
    "\n",
    "download_code(CHANNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb187678-a0db-42ad-99d8-4f1fd40d3160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configure\n",
    "\n",
    "Extract notebook properties into environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11026121-53b9-4916-9935-16f3cbd9fd13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure environment"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dbruntime.databricks_repl_context import get_context\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "VERSION=\"0.4\"\n",
    "\n",
    "os.environ[\"DATABRICKS_SDK_UPSTREAM\"] = \"databricks_vscode_tunnel\"\n",
    "os.environ[\"DATABRICKS_SDK_UPSTREAM_VERSION\"] = VERSION\n",
    "\n",
    "ctx = get_context()\n",
    "w = WorkspaceClient()\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = ctx.workspaceUrl\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = ctx.apiToken\n",
    "os.environ[\"DATABRICKS_CLUSTER_ID\"] = ctx.clusterId\n",
    "\n",
    "def get_notebook_dir():\n",
    "    path = os.path.dirname(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()) # type: ignore\n",
    "    return path if path.startswith(\"/Workspace\") else \"/Workspace\" + path\n",
    "\n",
    "DATABRICKS_USER_NAME = w.current_user.me().user_name\n",
    "\n",
    "notebook_dir = get_notebook_dir()\n",
    "os.environ[\"DATABRICKS_NOTEBOOK_DIR\"] = notebook_dir\n",
    "\n",
    "project_name = os.path.basename(notebook_dir).replace(\" \", \"-\")\n",
    "hash = hashlib.sha256(f\"{ctx.clusterId}-{project_name}\".encode()).hexdigest()[:6]\n",
    "DATABRICKS_ENV_NAME = project_name[:13] + \"-\" + hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35ffcc0-f993-4dea-b525-471a0fe8df62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configure Jupyter Kernel\n",
    "\n",
    "Expose `spark` and `dbutils` globals and add support for `%sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa257217-648a-4e59-bb63-e9467c73ce40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "init_script = \"\"\"\n",
    "\\\"\"\"Entry point for launching an IPython kernel with databricks feature support.\n",
    "\n",
    "This file is based on the kernel launcher from ipykernel[1]. In this launcher we initialize a\n",
    "connection to spark to both be used by user code and by databricks feature, initialize databricks\n",
    "setup that require setup on kernel start and launch the kernel app so the jupyter client can\n",
    "connect.\n",
    "\n",
    "[1] https://github.com/ipython/ipykernel/blob/v5.2.1/ipykernel_launcher.py\n",
    "\\\"\"\"\n",
    "\n",
    "from dbruntime.DatasetInfo import UserNamespaceDict\n",
    "from dbruntime.PipMagicOverrides import PipMagicOverrides\n",
    "from dbruntime.display import displayHTML\n",
    "from dbruntime.monkey_patches import apply_monkey_patches\n",
    "from dbruntime import UserNamespaceInitializer\n",
    "from dbruntime.IPythonShellHooks import load_ipython_hooks\n",
    "\n",
    "\n",
    "from IPython.core.getipython import get_ipython\n",
    "from IPython.display import display\n",
    "from dbruntime.IPythonShellHooks import IPythonShellHook\n",
    "\n",
    "user_namespace_initializer = UserNamespaceInitializer.getOrCreate()\n",
    "entry_point = user_namespace_initializer.get_spark_entry_point()\n",
    "\n",
    "sc = user_namespace_initializer.localSparkHandles[\"sc\"]\n",
    "spark = user_namespace_initializer.localSparkHandles[\"spark\"]\n",
    "dbutils = user_namespace_initializer.dbutils\n",
    "user_ns = UserNamespaceDict(\n",
    "    user_namespace_initializer.get_namespace_globals(),\n",
    "    entry_point.getDriverConf(),\n",
    "    entry_point,\n",
    ")\n",
    "\n",
    "shell = get_ipython()\n",
    "apply_monkey_patches(entry_point, sc, spark, display, displayHTML, dbutils)\n",
    "shell.register_magics(PipMagicOverrides(entry_point, sc._conf, user_ns))\n",
    "\n",
    "class UserNamespaceCommandHook(IPythonShellHook):\n",
    "    def __init__(self, user_ns):\n",
    "        self.user_ns = user_ns\n",
    "\n",
    "    def pre_run_cell(self, info):\n",
    "        self.user_ns.reset_new_dataframes()\n",
    "\n",
    "    def post_run_cell(self, result):\n",
    "        new_dataframe_info = self.user_ns.get_new_dataframe_infos_json()\n",
    "        if new_dataframe_info:\n",
    "            data = {\"text/plain\": new_dataframe_info}\n",
    "            display(data, raw=True)\n",
    "            \n",
    "load_ipython_hooks(shell, UserNamespaceCommandHook(user_ns))\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from IPython.core.formatters import BaseFormatter\n",
    "\n",
    "def register_magics():\n",
    "    def warn_for_dbr_alternative(magic: str):\n",
    "        # Magics that are not supported on Databricks but work in jupyter notebooks.\n",
    "        # We show a warning, prompting users to use a databricks equivalent instead.\n",
    "        local_magic_dbr_alternative = {\"%%sh\": \"%sh\"}\n",
    "        if magic in local_magic_dbr_alternative:\n",
    "            warnings.warn(\n",
    "                \"\\\\n\" + magic\n",
    "                + \" is not supported on Databricks. This notebook might fail when running on a Databricks cluster.\\\\n\"\n",
    "                  \"Consider using %\"\n",
    "                + local_magic_dbr_alternative[magic]\n",
    "                + \" instead.\"\n",
    "            )\n",
    "\n",
    "    def throw_if_not_supported(magic: str):\n",
    "        # These are magics that are supported on dbr but not locally.\n",
    "        unsupported_dbr_magics = [\"%r\", \"%scala\"]\n",
    "        if magic in unsupported_dbr_magics:\n",
    "            raise NotImplementedError(\n",
    "                magic\n",
    "                + \" is not supported for local Databricks Notebooks.\"\n",
    "            )\n",
    "\n",
    "    def is_cell_magic(lines: List[str]):\n",
    "        def get_cell_magic(lines: List[str]):\n",
    "            if len(lines) == 0:\n",
    "                return\n",
    "            if lines[0].strip().startswith(\"%%\"):\n",
    "                return lines[0].split(\" \")[0].strip()\n",
    "            \n",
    "        def handle(lines: List[str]):\n",
    "            cell_magic = get_cell_magic(lines)\n",
    "            if cell_magic is None:\n",
    "                return lines\n",
    "            warn_for_dbr_alternative(cell_magic)\n",
    "            throw_if_not_supported(cell_magic)\n",
    "            return lines\n",
    "\n",
    "        is_cell_magic.handle = handle\n",
    "        return get_cell_magic(lines) is not None\n",
    "\n",
    "    def is_line_magic(lines: List[str]):\n",
    "        def get_line_magic(lines: List[str]):\n",
    "            if len(lines) == 0:\n",
    "                return\n",
    "            if lines[0].strip().startswith(\"%\"):\n",
    "                return lines[0].split(\" \")[0].strip().strip(\"%\")\n",
    "            \n",
    "        def handle(lines: List[str]):\n",
    "            lmagic = get_line_magic(lines)\n",
    "            if lmagic is None:\n",
    "                return lines\n",
    "            warn_for_dbr_alternative(lmagic)\n",
    "            throw_if_not_supported(lmagic)\n",
    "\n",
    "            if lmagic == \"md\" or lmagic == \"md-sandbox\":\n",
    "                lines[0] = (\n",
    "                    \"%%markdown\" +\n",
    "                    lines[0].partition(\"%\" + lmagic)[2]\n",
    "                )\n",
    "                return lines\n",
    "\n",
    "            if lmagic == \"sql\":\n",
    "                lines = lines[1:]\n",
    "                spark_string = (\n",
    "                    \"global _sqldf\\\\n\"\n",
    "                    + \"_sqldf = spark.sql('''\"\n",
    "                    + \"\".join(lines).replace(\"'\", \"\\\\\\\\'\")\n",
    "                    + \"''')\\\\n\"\n",
    "                    + \"display(_sqldf)\\\\n\"\n",
    "                )\n",
    "                return spark_string.splitlines(keepends=True)\n",
    "\n",
    "            if lmagic == \"python\":\n",
    "                return lines[1:]\n",
    "\n",
    "        is_line_magic.handle = handle\n",
    "        return get_line_magic(lines) is not None\n",
    "        \n",
    "\n",
    "    def parse_line_for_databricks_magics(lines: List[str]):\n",
    "        if len(lines) == 0:\n",
    "            return lines\n",
    "        \n",
    "        lines = [line for line in lines \n",
    "                    if line.strip() != \"# Databricks notebook source\" and \\\\\n",
    "                    line.strip() != \"# COMMAND ----------\"\n",
    "                ]\n",
    "        lines = ''.join(lines).strip().splitlines(keepends=True)\n",
    "\n",
    "        for magic_check in [is_cell_magic, is_line_magic]:\n",
    "            if magic_check(lines):\n",
    "                return magic_check.handle(lines)\n",
    "\n",
    "        return lines\n",
    "\n",
    "    ip = get_ipython()\n",
    "    ip.input_transformers_cleanup.append(parse_line_for_databricks_magics)\n",
    "\n",
    "\n",
    "def register_formatters():\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def df_html(df):\n",
    "        return df.toPandas().to_html()\n",
    "\n",
    "    html_formatter = get_ipython().display_formatter.formatters[\"text/html\"]\n",
    "    html_formatter.for_type(DataFrame, df_html)\n",
    "\n",
    "    get_ipython().display_formatter.active_types.append('application/vnd.databricks.v1+datasetinfo')\n",
    "    get_ipython().display_formatter.formatters['application/vnd.databricks.v1+datasetinfo'] = get_ipython().display_formatter.formatters['text/plain'].__class__()\n",
    "    get_ipython().display_formatter.formatters['application/vnd.databricks.v1+datasetinfo'].enabled = True\n",
    "\n",
    "register_magics()\n",
    "register_formatters()\n",
    "\"\"\"\n",
    " \n",
    "os.makedirs(expanduser(\"~/.ipython/profile_default/startup\"), exist_ok=True)\n",
    "with open(expanduser(\"~/.ipython/profile_default/startup/init_script.py\"), \"w\") as f:\n",
    "    f.write(init_script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da49008a-c968-4ff8-990f-7801ea0f113f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Persist environment\n",
    "\n",
    "IDE extensions are ephemeral on disk and don't survive cluster restarts. Write `.vscode/extensions.json` so we can re-create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcda53c1-bd60-4502-9489-d4fb84f2d242",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Persist Settings"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "VSCODE_EXTENSIONS = [\n",
    "    \"ms-python.python\",\n",
    "    \"ms-toolsai.jupyter\",\n",
    "    \"donjayamanne.python-environment-manager\",\n",
    "    \"databricks.databricks\"\n",
    "]\n",
    "\n",
    "def persist_settings(extensions):\n",
    "    os.chdir(get_notebook_dir())\n",
    "    \n",
    "    if not os.path.exists(\".vscode/extensions.json\"):\n",
    "        os.makedirs(\".vscode\", exist_ok=True)\n",
    "        with open(\".vscode/extensions.json\", \"w\") as f:\n",
    "            f.write(\"\"\"{\n",
    "        \"recommendations\": %s\n",
    "    }\n",
    "    \"\"\" % json.dumps(extensions))\n",
    "\n",
    "    if not os.path.exists(\".vscode/settings.json\"):\n",
    "        os.makedirs(\".vscode\", exist_ok=True)\n",
    "        with open(\".vscode/settings.json\", \"w\") as f:\n",
    "            f.write(\"{}\")\n",
    "\n",
    "    with open(\".vscode/settings.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data[\"python.defaultInterpreterPath\"] = sys.executable\n",
    "        with open(\".vscode/settings.json\", \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "persist_settings(VSCODE_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bef4348-6587-4c2b-af19-56f0e9e02319",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Persist Login Token"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "import shutil\n",
    "\n",
    "def symlink_force(source, target):\n",
    "    try:\n",
    "        os.remove(target)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    os.symlink(source, target, target_is_directory=False)\n",
    "\n",
    "def persist_login_token(user_name):\n",
    "    # Define the base paths\n",
    "    user_base_path = f\"/Workspace/Users/{user_name}/.vscode/cli\"\n",
    "    root_base_path_vscode = expanduser(\"~/.vscode/cli\")\n",
    "    root_base_path_vscode_insiders = expanduser(\"~/.vscode-insiders/cli\")\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(user_base_path, exist_ok=True)\n",
    "    os.makedirs(root_base_path_vscode, exist_ok=True)\n",
    "    os.makedirs(root_base_path_vscode_insiders, exist_ok=True)\n",
    "\n",
    "    # Path to the token file\n",
    "    token_file_path = os.path.join(user_base_path, \"token.json\")\n",
    "\n",
    "    # Ensure the token file exists\n",
    "    open(token_file_path, 'a').close()\n",
    "\n",
    "    # Create symbolic links\n",
    "    symlink_force(token_file_path, os.path.join(root_base_path_vscode, \"token.json\"))\n",
    "    symlink_force(token_file_path, os.path.join(root_base_path_vscode_insiders, \"token.json\"))\n",
    "\n",
    "persist_login_token(DATABRICKS_USER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6357e3-9db4-4112-8a4e-d8928dc96923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example Notebook\n",
    "\n",
    "Optionally create example notebook to be used in VS Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13f56cc-1cdb-4ec9-ba64-7b2278753df5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example Notebook"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def write_example_notebook():\n",
    "    example = r\"\"\"{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Use Spark\\n\",\n",
    "    \"\\n\",\n",
    "    \"* directly use `spark` global\\n\",\n",
    "    \"* Spark through Databricks Connect (just like in shared clusters)\\n\",\n",
    "    \"* set breakpoints and use step debugging\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df = spark.table(\\\"samples.nyctaxi.trips\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"display(df.limit(10))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# DBUtils\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Supports common subset of DBUtils features\\n\",\n",
    "    \"  * `dbutils.fs`\\n\",\n",
    "    \"  * `dbutils.secrets`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(\\\"Files:\\\")\\n\",\n",
    "    \"for file in dbutils.fs.ls(\\\"/\\\")[:5]:\\n\",\n",
    "    \"    print(file.path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print()\\n\",\n",
    "    \"print(\\\"Secret Scopes\\\")\\n\",\n",
    "    \"for scope in dbutils.secrets.listScopes()[:5]:\\n\",\n",
    "    \"    print(scope.name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# SQL\\n\",\n",
    "    \"\\n\",\n",
    "    \"Execute SQL using `%sql`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"vscode\": {\n",
    "     \"languageId\": \"sql\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"%sql\\n\",\n",
    "    \"\\n\",\n",
    "    \"select * from samples.nyctaxi.trips limit 10\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# GPUs (MLR only)\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Leverage GPUs\\n\",\n",
    "    \"* Use ML libraries such as `pytorch` from ML Runtimes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"for i in range(torch.cuda.device_count()):\\n\",\n",
    "    \"   print(torch.cuda.get_device_properties(i).name)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"pythonEnv-bea1ee52-5030-4e6d-a542-e3495d74414d\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.0rc1\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(get_notebook_dir())\n",
    "    if not os.path.exists(\"example.ipynb\"):\n",
    "        with open(\"example.ipynb\", \"w\") as f:\n",
    "            f.write(example)\n",
    "\n",
    "if CREATE_EXAMPLE_NOTEBOOK:\n",
    "    write_example_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d43995-e3e4-4dfb-a9f9-425e7f547666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start Tunnel\n",
    "\n",
    "Start the VS Code tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660a6be8-d282-4b0f-9df1-ce54f288cffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing tunnel after 30m\n",
      "*\n",
      "* Visual Studio Code Server\n",
      "*\n",
      "* By using the software, you agree to\n",
      "* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and\n",
      "* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n",
      "*\n",
      "[2025-01-17 14:45:34] warn Command-line options will not be applied until the existing tunnel exits.\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 35267 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 34373 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 33651 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 35059 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 44269 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 40201 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 45337 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 41381 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 41053 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 36877 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 40673 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 40137 (public=false)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Forwarding port 57939 (public=false)\n",
      "[2025-01-17 14:45:34] warn [rpc.0] error handling call: NoAttachedServerError(NoAttachedServerError)\n",
      "[2025-01-17 14:45:34] info [rpc.0] Disposed of connection to running server.\n",
      "Connected to an existing tunnel process running on this machine.\n",
      "\n",
      "Open this link in your browser https://insiders.vscode.dev/tunnel/dais-cow-bff-ef5256/Workspace/Users/fabian.jakobs@databricks.com/dais-cow-bff\n",
      "\n",
      "[2025-01-17 14:46:33] info [tunnels::connections::relay_tunnel_host] Opened new client on channel 3\n",
      "[2025-01-17 14:46:33] info [russh::server] wrote id\n",
      "[2025-01-17 14:46:33] info [russh::server] read other id\n",
      "[2025-01-17 14:46:33] info [russh::server] session is running\n",
      "[2025-01-17 14:46:34] info [rpc.2] Checking /root/.vscode-insiders/cli/servers/Insiders-a74aabd9bb87c33b047c822aa79d265bc5f5543e/log.txt and /root/.vscode-insiders/cli/servers/Insiders-a74aabd9bb87c33b047c822aa79d265bc5f5543e/pid.txt for a running server...\n",
      "[2025-01-17 14:46:34] info [rpc.2] Found running server (pid=25096)\n",
      "[2025-01-17 14:46:42] info [rpc.2] Forwarding port 35267 (public=false)\n",
      "[2025-01-17 14:46:42] info [rpc.2] Forwarding port 34373 (public=false)\n",
      "[2025-01-17 14:46:42] info [rpc.2] Forwarding port 33651 (public=false)\n",
      "[2025-01-17 14:46:43] info [rpc.2] Forwarding port 35059 (public=false)\n",
      "[2025-01-17 14:46:43] info [rpc.2] Forwarding port 44269 (public=false)\n",
      "[2025-01-17 14:46:44] info [rpc.2] Forwarding port 40201 (public=false)\n",
      "[2025-01-17 14:46:44] info [rpc.2] Forwarding port 45337 (public=false)\n",
      "[2025-01-17 14:46:44] info [rpc.2] Forwarding port 41381 (public=false)\n",
      "[2025-01-17 14:46:44] info [rpc.2] Forwarding port 41053 (public=false)\n",
      "[2025-01-17 14:46:47] info [rpc.2] Forwarding port 43895 (public=false)\n",
      "[2025-01-17 14:46:50] info [rpc.2] Forwarding port 43249 (public=false)\n",
      "[2025-01-17 14:46:52] info [rpc.2] Forwarding port 54533 (public=false)\n",
      "[2025-01-17 14:46:53] info [rpc.2] Forwarding port 43761 (public=false)\n",
      "[2025-01-17 14:47:21] warn [rpc.2] error handling call: NoAttachedServerError(NoAttachedServerError)\n",
      "[2025-01-17 14:47:21] info [rpc.2] Disposed of connection to running server.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:464)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:51)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:51)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:553)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:839)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:865)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:864)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:919)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:712)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:728)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:446)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:446)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:464)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:571)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:51)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:51)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:553)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:839)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:865)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:864)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:919)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:712)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:545)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:48)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:545)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:523)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:175)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:105)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:110)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:107)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:87)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "from os.path import expanduser\n",
    "\n",
    "def start_tunnel(channel, timeout, provider, extensions, tunnel_name):\n",
    "    if channel == \"insider\":\n",
    "        cli = expanduser(\"~/code/code-insiders\")\n",
    "    else:\n",
    "        cli = expanduser(\"~/code/code\")\n",
    "\n",
    "    # Check if the user is logged in\n",
    "    user_status = subprocess.getoutput(f\"{cli} tunnel user show\")\n",
    "    if user_status == \"not logged in\":\n",
    "        subprocess.run(\n",
    "            [cli, \"tunnel\", \"user\", \"login\", \"--provider\", provider], check=True\n",
    "        )\n",
    "\n",
    "    # Prepare the extensions argument\n",
    "    ext_args = []\n",
    "    for ext in extensions:\n",
    "        ext_args.extend([\"--install-extension\", ext])\n",
    "\n",
    "    # Kill the tunnel after a specified duration\n",
    "    print(f\"Killing tunnel after {timeout}\")\n",
    "\n",
    "    # Run the subprocess and forward stdout and stderr\n",
    "    command = [\n",
    "        \"timeout\",\n",
    "        f\"{timeout}\",\n",
    "        cli,\n",
    "        \"tunnel\",\n",
    "        *ext_args,\n",
    "        \"--accept-server-license-terms\",\n",
    "        \"--name\",\n",
    "        tunnel_name,\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
    "    )\n",
    "\n",
    "    # Forward stdout and stderr to the main process\n",
    "    for line in process.stdout:\n",
    "        print(line, end=\"\")  # Forward stdout line to main process stdout\n",
    "\n",
    "    print(\"Tunnel closed!\")\n",
    "\n",
    "start_tunnel(CHANNEL, DURATION, PROVIDER, VSCODE_EXTENSIONS, DATABRICKS_ENV_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6140001546917352,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "VS Code Tunnel",
   "widgets": {
    "Create example.ipynb": {
     "currentValue": "Yes",
     "nuid": "8b2586b8-1dcf-4f76-96de-17b5ba581002",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Yes",
      "label": null,
      "name": "Create example.ipynb",
      "options": {
       "choices": [
        "Yes",
        "No"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "Yes",
      "label": null,
      "name": "Create example.ipynb",
      "options": {
       "autoCreated": null,
       "choices": [
        "Yes",
        "No"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Duration": {
     "currentValue": "30m",
     "nuid": "94c23f2d-af06-4cdb-9be5-50025a3541a4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10m",
      "label": null,
      "name": "Duration",
      "options": {
       "choices": [
        "10m",
        "30m",
        "1h",
        "4h"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "10m",
      "label": null,
      "name": "Duration",
      "options": {
       "autoCreated": null,
       "choices": [
        "10m",
        "30m",
        "1h",
        "4h"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Provider": {
     "currentValue": "microsoft",
     "nuid": "8b29da90-2ad1-4c75-bca1-ba9bcb3f3f3e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "microsoft",
      "label": null,
      "name": "Provider",
      "options": {
       "choices": [
        "github",
        "microsoft"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "microsoft",
      "label": null,
      "name": "Provider",
      "options": {
       "autoCreated": null,
       "choices": [
        "github",
        "microsoft"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "VS Code Channel": {
     "currentValue": "insider",
     "nuid": "7c020965-fcca-4067-bfb3-2680844a8124",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "stable",
      "label": null,
      "name": "VS Code Channel",
      "options": {
       "choices": [
        "stable",
        "insider"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "stable",
      "label": null,
      "name": "VS Code Channel",
      "options": {
       "autoCreated": null,
       "choices": [
        "stable",
        "insider"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
